{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hacbook/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from time import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import time\n",
    "from torchvision import transforms\n",
    "# import pyttsx3\n",
    "import torch_pruning as tp\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import thop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch_pruning\n",
    "# pip install playsound\n",
    "# pip install opencv-python\n",
    "# pip install torchsummary\n",
    "# pip install thop\n",
    "# pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset_Class\n",
    "class PyTorch_Custom_Dataset_Class(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class PyTorch_Classification_Dataset_Class(Dataset):\n",
    "    def __init__(self, dataset_dir=\"/home/jetson/Downloads/project/Recycle_Classification_Dataset\"):\n",
    "        super().__init__()\n",
    "        self.image_abs_path = os.path.abspath(dataset_dir)\n",
    "        self.label_list = [d for d in os.listdir(self.image_abs_path) if os.path.isdir(os.path.join(self.image_abs_path, d))]\n",
    "        self.label_list.sort()\n",
    "        self.x_list = []  \n",
    "        self.y_list = []  \n",
    "        for label_index, label_str in enumerate(self.label_list):\n",
    "            img_path = os.path.join(self.image_abs_path, label_str)\n",
    "            img_list = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "            for img in img_list:\n",
    "                self.x_list.append(os.path.join(img_path, img))\n",
    "                self.y_list.append(label_index)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.x_list[idx] \n",
    "        label = self.y_list[idx] \n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\") \n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "    def __save_label_map__(self, dst_text_path=\"label_map.txt\"):\n",
    "        label_list = self.label_list\n",
    "        with open(dst_text_path, 'w') as f:\n",
    "            for label in label_list:\n",
    "                f.write(f\"{label}\\n\")\n",
    "\n",
    "    def __num_classes__(self):\n",
    "        return len(self.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_Class_From_the_Scratch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorch_Custom_Model_Class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(batch, channels)\n",
    "        y = self.fc(y).view(batch, channels, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class BottleneckResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
    "        self.se = SEBlock(in_channels, reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = self.se(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class MODEL_From_Scratch(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),  \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(64)\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            BottleneckResidualBlock(64, 128, reduction=16),\n",
    "            BottleneckResidualBlock(64, 128, reduction=16),\n",
    "            BottleneckResidualBlock(64, 128, reduction=16)  \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_Class\n",
    "class PyTorch_Classification_Training_Class():\n",
    "    def __init__(self\n",
    "                , dataset_dir = \"/home/jetson/Downloads/project/Recycle_Classification_Dataset\"\n",
    "                , batch_size = 16\n",
    "                , train_ratio = 0.75\n",
    "                ):\n",
    "        \n",
    "        if not os.path.isdir(dataset_dir) or not os.listdir(dataset_dir):\n",
    "            if os.path.isdir(dataset_dir):\n",
    "                print(f\"Directory {dataset_dir} exists but is empty. Cloning dataset...\")\n",
    "        else:\n",
    "            print(f\"Directory {dataset_dir} already exists and is not empty. Using existing dataset.\")\n",
    "  \n",
    "        self.DEVICE =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.DEVICE}\")\n",
    "\n",
    "        dataset = PyTorch_Classification_Dataset_Class(dataset_dir = dataset_dir)\n",
    "        dataset.__save_label_map__()\n",
    "        self.num_classes = dataset.__num_classes__()\n",
    "        train_size = int(train_ratio * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset\n",
    "            , batch_size=batch_size\n",
    "            , shuffle=True\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset\n",
    "            , batch_size=batch_size\n",
    "            , shuffle=False\n",
    "        )\n",
    "        self.model = None\n",
    "        self.model_str = None\n",
    "        \n",
    "    def prepare_network(self):\n",
    "        self.model = MODEL_From_Scratch(self.num_classes)\n",
    "        self.model.to(self.DEVICE)\n",
    "        self.model_str = \"PyTorch_Training_From_Scratch.pth\"\n",
    "\n",
    "    def training_network(self\n",
    "            , learning_rate = 0.0001\n",
    "            , epochs = 10\n",
    "            , step_size = 3\n",
    "            , gamma = 0.3):\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        acc = 0.0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            for data, target in tqdm(self.train_loader):\n",
    "                data, target = data.to(self.DEVICE), target.to(self.DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            self.model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(tqdm(self.test_loader)):\n",
    "                    data, target = data.to(self.DEVICE), target.to(self.DEVICE)\n",
    "                    output = self.model(data)\n",
    "                    batch_loss = F.cross_entropy(output, target, reduction='sum').item()\n",
    "                    test_loss += batch_loss\n",
    "                    pred = output.max(1, keepdim=True)[1]\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss /= len(self.test_loader.dataset)\n",
    "            test_accuracy = 100.0 * correct / len(self.test_loader.dataset)\n",
    "            print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch, test_loss, test_accuracy))\n",
    "\n",
    "\n",
    "            if acc < test_accuracy or epoch == epochs:\n",
    "                acc = test_accuracy\n",
    "                torch.save(self.model.state_dict(), self.model_str)\n",
    "                print(\"model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_class = PyTorch_Classification_Training_Class()\n",
    "    training_class.prepare_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_class.training_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"/home/jetson/Downloads/PyTorch_Training_From_Scratch.pth\"\n",
    "\n",
    "model = copy.deepcopy(training_class.model)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image = image.resize((128, 128))\n",
    "    image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "    image_np = (image_np - np.array([0.485, 0.456, 0.406], dtype=np.float32)) / np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    image_tensor = torch.tensor(image_np, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=30,\n",
    "    flip_method=0\n",
    "):\n",
    "    return (\n",
    "        f\"v4l2src device=/dev/video0 ! video/x-raw, width={capture_width}, height={capture_height}, framerate={framerate}/1 ! \"\n",
    "f\"videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "\n",
    "    )\n",
    "\n",
    "def predict_class(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(image_tensor)\n",
    "        inference_time = time.time() - start_time\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item(), inference_time\n",
    "\n",
    "def load_labels(label_map_path=\"label_map.txt\"):\n",
    "    with open(label_map_path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "    return labels\n",
    "\n",
    "def speak(text, language=\"ko\", filename=\"output.mp3\"):\n",
    "    tts = gTTS(text=text, lang=language)\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    model = MODEL_From_Scratch(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def run_inference(model_path, label_map_path=\"label_map.txt\"):\n",
    "    labels = load_labels(label_map_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    pipeline = (\n",
    "    \"nvarguscamerasrc ! video/x-raw(memory:NVMM),format=NV12,width=640,height=480,framerate=30/1 ! \"\n",
    "    \"nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1\")\n",
    "\n",
    "    cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)\n",
    "    if not cap.isOpened():\n",
    "        print(\"카메라를 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"카메라가 실행 중입니다. 's' 키를 눌러 분류를 실행하세요. 'q' 키를 눌러 종료하세요.\")\n",
    "    inference_times = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == ord(\"s\"):\n",
    "            image_tensor = preprocess_image(frame)\n",
    "\n",
    "            predicted_class, inference_time = predict_class(model, image_tensor, device)\n",
    "            predicted_label = labels[predicted_class]\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            print(f\"Inference Time: {inference_time:.4f}s\")\n",
    "            \n",
    "            cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "            speak(f\"으으으으 으으으으{predicted_label}으로 분류하세요\")\n",
    "\n",
    "        elif key == ord(\"q\"): \n",
    "            if inference_times:\n",
    "                avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "                print(f\"Average Inference Time: {avg_inference_time:.4f}s\")\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 실행\n",
    "run_inference(\"/home/jetson/Downloads/PyTorch_Training_From_Scratch.pth\", \"label_map.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_loader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inference_time = []\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            inference_time.append(time.time() - start_time)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    avg_inference_time = sum(inference_time) / len(inference_time) if inference_time else 0\n",
    "    return accuracy, avg_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, t = test_accuracy(copy.deepcopy(training_class.model), training_class.test_loader, training_class.DEVICE)\n",
    "print(f\"Test Accuracy: {accuracy:.5f}%\")\n",
    "print(f\"Inference Time: {t:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(training_class.model, (3, 128, 128), device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macs, params = tp.utils.count_ops_and_params(copy.deepcopy(training_class.model), torch.zeros(1, 3, 26, 34).to('cuda'))\n",
    "print(f'Basic model Params: {params/1e6} M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_execution_time(model, example_inputs, device, iterations=30):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "            _ = model(example_inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "    avg_time = (end_time - start_time) / iterations\n",
    "    return avg_time\n",
    "\n",
    "def compare_models(base_model, light_model, example_inputs, test_loader, device):\n",
    "    base_macs, base_params = tp.utils.count_ops_and_params(base_model, example_inputs)\n",
    "    light_macs, light_params = tp.utils.count_ops_and_params(light_model, example_inputs)\n",
    "\n",
    "    base_flops = 2 * base_macs\n",
    "    light_flops = 2 * light_macs\n",
    "\n",
    "    base_time = measure_execution_time(base_model, example_inputs, device)\n",
    "    light_time = measure_execution_time(light_model, example_inputs, device)\n",
    "\n",
    "    base_accuracy, _ = test_accuracy(base_model, test_loader, device)\n",
    "    light_accuracy, _ = test_accuracy(light_model, test_loader, device)\n",
    "\n",
    "    print(\"\\nComparison of Models:\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Metric                  | Base Model  | light Model\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Parameters (M)          | {base_params / 1e6:.3f}       | {light_params / 1e6:.3f}\")\n",
    "    print(f\"FLOPs (M)               | {base_flops / 1e6:.2f}       | {light_flops / 1e6:.2f}\")\n",
    "    print(f\"Execution Time (ms)     | {base_time * 1e3:.3f}        | {light_time * 1e3:.3f}\")\n",
    "    print(f\"Accuracy (%)            | {base_accuracy:.2f}       | {light_accuracy:.2f}\")\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DependencyGraph를 기반으로 의존성을 가지는 연산들까지 Pruning을 적용하는 함수.\n",
    "def prune_with_dependency_conv(model, example_inputs, ratio, device=\"cuda\"):\n",
    "    \n",
    "    model = copy.deepcopy(model).to(device)\n",
    "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "    conv_layers = [mod for mod in DG.module2node.keys() if isinstance(mod, nn.Conv2d)]\n",
    "\n",
    "    for layer in conv_layers:\n",
    "        num_channels = layer.out_channels\n",
    "        num_pruned = int(num_channels * ratio)\n",
    "        if num_pruned <= 0:\n",
    "            continue  \n",
    "        pruning_idxs = list(range(num_pruned))\n",
    "\n",
    "        pruning_group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, idxs=pruning_idxs)\n",
    "        \n",
    "        if DG.check_pruning_group(pruning_group):\n",
    "            pruning_group.prune()\n",
    "\n",
    "    p_acc, p_time = test_accuracy(model, training_class.test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prune_macs, prune_params = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "    return model, p_acc, prune_params, 2*prune_macs, p_time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "results1 = []\n",
    "sparsities = [0.0, 0.25, 0.50, 0.75]\n",
    "example_inputs = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "for s in tqdm(sparsities):\n",
    "    save_path = f\"pruned_model_conv{int(s * 100)}.pt\"\n",
    "    pruned_model, p_acc, p_params, p_flops, p_time = prune_with_dependency_conv(training_class.model, example_inputs, s, device=device)\n",
    "    torch.save(pruned_model.state_dict(), save_path)\n",
    "\n",
    "    results1.append({\n",
    "        \"sparsity\": s,\n",
    "        \"accuracy\": p_acc,\n",
    "        \"params (M)\": p_params,\n",
    "        \"FLOPs (M)\": p_flops,\n",
    "        \"time (s)\": p_time\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results1)\n",
    "\n",
    "print(\"\\nPruning Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DependencyGraph를 기반으로 의존성을 가지는 연산들까지 Pruning을 적용하는 함수.\n",
    "def prune_with_dependency_linear(model, example_inputs, ratio, device=\"cuda\"):\n",
    "    \n",
    "    model = copy.deepcopy(model).to(device)\n",
    "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "    linear_layers = [mod for mod in DG.module2node.keys() if isinstance(mod, nn.Linear)]\n",
    "\n",
    "    for layer in linear_layers:\n",
    "        num_neurons = layer.out_features\n",
    "        num_pruned = int(num_neurons * ratio)\n",
    "        if num_pruned <= 0:\n",
    "            continue\n",
    "        pruning_idxs = list(range(num_pruned))\n",
    "\n",
    "        pruning_group = DG.get_pruning_group(layer, tp.prune_linear_out_channels, idxs=pruning_idxs)\n",
    "        if DG.check_pruning_group(pruning_group):\n",
    "            pruning_group.prune()\n",
    "\n",
    "    p_acc, p_time = test_accuracy(model, training_class.test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    prune_macs, prune_params = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "    return model, p_acc, prune_params, 2*prune_macs, p_time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "example_inputs = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "results2 = []\n",
    "sparsities = [0.0, 0.25, 0.50, 0.75]\n",
    "for s in tqdm(sparsities):\n",
    "    save_path = f\"pruned_model_linear{int(s * 100)}.pt\"\n",
    "    pruned_model2, p_acc2, p_params2, p_flops2, p_time2 = prune_with_dependency_linear(training_class.model, example_inputs, s, device=device)\n",
    "    torch.save(pruned_model2.state_dict(), save_path)\n",
    "\n",
    "    results2.append({\n",
    "        \"sparsity\": s,\n",
    "        \"accuracy\": p_acc2,\n",
    "        \"params (M)\": p_params2,\n",
    "        \"FLOPs (M)\": p_flops2,\n",
    "        \"time (s)\": p_time2\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results2)\n",
    "\n",
    "print(\"\\nPruning Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_with_dependency_all(model, example_inputs, ratio_conv, ratio_linear, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \n",
    "    model = copy.deepcopy(model).to(device)\n",
    "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "    \n",
    "    conv_layers = [mod for mod in DG.module2node.keys() if isinstance(mod, nn.Conv2d)]\n",
    "    for layer in conv_layers:\n",
    "        num_channels = layer.out_channels\n",
    "        num_pruned = int(num_channels * ratio_conv)\n",
    "        if num_pruned <= 0:\n",
    "            continue\n",
    "        pruning_idxs = list(range(num_pruned))\n",
    "        pruning_group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, idxs=pruning_idxs)\n",
    "        if DG.check_pruning_group(pruning_group):\n",
    "            pruning_group.prune()\n",
    "    \n",
    "    linear_layers = [mod for mod in DG.module2node.keys() if isinstance(mod, nn.Linear)]\n",
    "    for layer in linear_layers:\n",
    "        num_neurons = layer.out_features\n",
    "        num_pruned = int(num_neurons * ratio_linear)\n",
    "        if num_pruned <= 0:\n",
    "            continue\n",
    "        pruning_idxs = list(range(num_pruned))\n",
    "        pruning_group = DG.get_pruning_group(layer, tp.prune_linear_out_channels, idxs=pruning_idxs)\n",
    "        if DG.check_pruning_group(pruning_group):\n",
    "            pruning_group.prune()\n",
    "    \n",
    "    p_acc, p_time = test_accuracy(model, training_class.test_loader, device=device)\n",
    "    prune_macs, prune_params = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "    return model, p_acc, prune_params, 2*prune_macs, p_time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "example_inputs = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "results_all = []\n",
    "sparsities = [0.0, 0.25, 0.50, 0.75]\n",
    "\n",
    "for s in tqdm(sparsities):\n",
    "    save_path = f\"pruned_model_all_{int(s * 100)}.pt\"\n",
    "    pruned_model, p_acc, p_params, p_flops, p_time = prune_with_dependency_all(\n",
    "        model=training_class.model, \n",
    "        example_inputs=example_inputs, \n",
    "        ratio_conv=s, \n",
    "        ratio_linear=s, \n",
    "        device=device\n",
    "    )\n",
    "    torch.save(pruned_model.state_dict(), save_path)\n",
    "    \n",
    "    results_all.append({\n",
    "        \"sparsity\": s,\n",
    "        \"accuracy\": p_acc,\n",
    "        \"params (M)\": p_params,\n",
    "        \"FLOPs (M)\": p_flops,\n",
    "        \"time (s)\": p_time\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_all)\n",
    "\n",
    "print(\"\\nCombined Pruning Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_pruned_model(model_path, device):\n",
    "#     p_model = copy.deepcopy(training_class.model)\n",
    "#     p_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#     p_model.to(device)\n",
    "#     p_model.eval()  \n",
    "#     return p_model\n",
    "\n",
    "# def calculate_flops_and_params(model, example_inputs):\n",
    "#     # flops, params = thop.profile(model, inputs=(example_inputs,), verbose=False)\n",
    "#     prune_macs, prune_params = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "#     return 2*prune_macs, prune_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image = image.resize((128, 128))\n",
    "    image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "    image_np = (image_np - np.array([0.485, 0.456, 0.406], dtype=np.float32)) / np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    image_tensor = torch.tensor(image_np, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=30,\n",
    "    flip_method=0\n",
    "):\n",
    "    return (\n",
    "        f\"v4l2src device=/dev/video0 ! video/x-raw, width={capture_width}, height={capture_height}, framerate={framerate}/1 ! \"\n",
    "f\"videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "\n",
    "    )\n",
    "\n",
    "def predict_class(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(image_tensor)\n",
    "        inference_time = time.time() - start_time\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item(), inference_time\n",
    "\n",
    "def load_labels(label_map_path=\"label_map.txt\"):\n",
    "    with open(label_map_path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "    return labels\n",
    "\n",
    "def speak(text, language=\"ko\", filename=\"output.mp3\"):\n",
    "    tts = gTTS(text=text, lang=language)\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    model =  copy.deepcopy(training_class.model)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def run_inference(model_path, label_map_path=\"label_map.txt\"):\n",
    "    labels = load_labels(label_map_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    pipeline = (\n",
    "    \"nvarguscamerasrc ! video/x-raw(memory:NVMM),format=NV12,width=640,height=480,framerate=30/1 ! \"\n",
    "    \"nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1\")\n",
    "\n",
    "    cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)\n",
    "    if not cap.isOpened():\n",
    "        print(\"카메라를 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"카메라가 실행 중입니다. 's' 키를 눌러 분류를 실행하세요. 'q' 키를 눌러 종료하세요.\")\n",
    "    inference_times = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == ord(\"s\"):\n",
    "            image_tensor = preprocess_image(frame)\n",
    "\n",
    "            predicted_class, inference_time = predict_class(model, image_tensor, device)\n",
    "            predicted_label = labels[predicted_class]\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            print(f\"Inference Time: {inference_time:.4f}s\")\n",
    "            \n",
    "            cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "            speak(f\"으으으으 으으으으{predicted_label}으로 분류하세요\")\n",
    "\n",
    "        elif key == ord(\"q\"): \n",
    "            if inference_times:\n",
    "                avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "                print(f\"Average Inference Time: {avg_inference_time:.4f}s\")\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\"/home/jetson/Downloads/pruned_model_all_75.pt\", \"label_map.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(StudentModel, self).__init__()\n",
    "        \n",
    "        def depthwise_separable_conv(in_channels, out_channels, stride=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.block1 = depthwise_separable_conv(32, 64, stride=1)\n",
    "        self.block2 = depthwise_separable_conv(64, 128, stride=2)\n",
    "        self.block3 = depthwise_separable_conv(128, 256, stride=2)\n",
    "        self.block4 = depthwise_separable_conv(256, 512, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def kd_loss(student_outputs, teacher_outputs, temperature):\n",
    "    teacher_probs = torch.softmax(teacher_outputs / temperature, dim=1)\n",
    "    student_probs = torch.log_softmax(student_outputs / temperature, dim=1)\n",
    "    loss = nn.KLDivLoss(reduction='batchmean')(student_probs, teacher_probs)\n",
    "    return loss * (temperature ** 2)\n",
    "\n",
    "def combined_loss(student_outputs, targets, teacher_outputs, criterion, temperature, alpha):\n",
    "    hard_loss = criterion(student_outputs, targets)\n",
    "    soft_loss = kd_loss(student_outputs, teacher_outputs, temperature)\n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "def train_student_with_kd(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    temperature=3.0,\n",
    "    alpha=0.5\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher_model = teacher_model.to(device)\n",
    "    student_model = student_model.to(device)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            student_outputs = student_model(inputs)\n",
    "\n",
    "            loss = combined_loss(student_outputs, targets, teacher_outputs, criterion, temperature, alpha)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "                student_outputs = student_model(inputs)\n",
    "\n",
    "                loss = combined_loss(student_outputs, targets, teacher_outputs, criterion, temperature, alpha)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(student_outputs, 1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/jetson/Downloads/project/Recycle_Classification_Dataset\"\n",
    "dataset = PyTorch_Classification_Dataset_Class(dataset_dir=dataset_dir)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "teacher_model = copy.deepcopy(training_class.model)\n",
    "student_model = StudentModel(num_classes=dataset.__num_classes__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_student_model = train_student_with_kd(\n",
    "#     teacher_model=teacher_model,\n",
    "#     student_model=student_model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     epochs=10,\n",
    "#     learning_rate=0.001,\n",
    "#     temperature=3.0,\n",
    "#     alpha=0.5\n",
    "# )\n",
    "\n",
    "# torch.save(trained_student_model.state_dict(), \"lightweight_student_model.pth\")\n",
    "# print(\"Student model saved as 'lightweight_student_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_student_model = StudentModel(2)\n",
    "trained_student_model.load_state_dict(torch.load(\"/home/jetson/Downloads/lightweight_student_model.pth\", map_location=device))\n",
    "\n",
    "trained_student_model = trained_student_model.to(device).to(dtype=torch.float32) \n",
    "teacher_model = copy.deepcopy(training_class.model).to(device).to(dtype=torch.float32) \n",
    "\n",
    "example_inputs = torch.randn(1, 3, 128, 128, dtype=torch.float32).to(device)\n",
    "compare_models(teacher_model, trained_student_model, example_inputs, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image = image.resize((128, 128))\n",
    "    image_np = np.array(image, dtype=np.float32) / 255.0\n",
    "    image_np = (image_np - np.array([0.485, 0.456, 0.406], dtype=np.float32)) / np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    image_tensor = torch.tensor(image_np, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    capture_width=1280,\n",
    "    capture_height=720,\n",
    "    display_width=1280,\n",
    "    display_height=720,\n",
    "    framerate=30,\n",
    "    flip_method=0\n",
    "):\n",
    "    return (\n",
    "        f\"v4l2src device=/dev/video0 ! video/x-raw, width={capture_width}, height={capture_height}, framerate={framerate}/1 ! \"\n",
    "f\"videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )\n",
    "\n",
    "def predict_class(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(image_tensor)\n",
    "        inference_time = time.time() - start_time\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item(), inference_time\n",
    "\n",
    "def load_labels(label_map_path=\"label_map.txt\"):\n",
    "    with open(label_map_path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "    return labels\n",
    "\n",
    "def speak(text, language=\"ko\", filename=\"output.mp3\"):\n",
    "    tts = gTTS(text=text, lang=language)\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    model = StudentModel(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def run_inference(model_path, label_map_path=\"label_map.txt\"):\n",
    "    labels = load_labels(label_map_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    pipeline = (\n",
    "    \"nvarguscamerasrc ! video/x-raw(memory:NVMM),format=NV12,width=640,height=480,framerate=30/1 ! \"\n",
    "    \"nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1\")\n",
    "\n",
    "    cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)\n",
    "    if not cap.isOpened():\n",
    "        print(\"카메라를 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"카메라가 실행 중입니다. 's' 키를 눌러 분류를 실행하세요. 'q' 키를 눌러 종료하세요.\")\n",
    "    inference_times = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == ord(\"s\"):\n",
    "            image_tensor = preprocess_image(frame)\n",
    "\n",
    "            predicted_class, inference_time = predict_class(model, image_tensor, device)\n",
    "            predicted_label = labels[predicted_class]\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            print(f\"Inference Time: {inference_time:.4f}s\")\n",
    "            \n",
    "            cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "            speak(f\"으으으으 으으으으{predicted_label}으로 분류하세요\")\n",
    "\n",
    "        elif key == ord(\"q\"): \n",
    "            if inference_times:\n",
    "                avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "                print(f\"Average Inference Time: {avg_inference_time:.4f}s\")\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\"/home/jetson/Downloads/lightweight_student_model.pth\", \"label_map.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
